{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"try","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNGpLTU+/SOtEnUQrEgpWMW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"aCIQHtxwoQ3v","colab_type":"code","colab":{}},"source":["from google.colab import drive\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6mieeZmjpUNp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1593947059490,"user_tz":-330,"elapsed":38091,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"3bf19cef-d6fe-4943-84b4-bebfd009cf82"},"source":["drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y12Zu2s3pWAF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593947064397,"user_tz":-330,"elapsed":1204,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"9f663bbf-ec66-4b07-a495-0f0a4c79f379"},"source":["cd gdrive/My Drive"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1Tq1can7pgNK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593947070735,"user_tz":-330,"elapsed":3244,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"679d044a-5606-4b8e-97d0-f8b699db59c4"},"source":["ls\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" 00811502717_Deepak_Dhingra.pdf\n","'16 by 16 orthogonal maze.pdf'\n"," 475039ab-a8a7-497f-a62a-e6f6266902c1.MP4\n","\u001b[0m\u001b[01;34m'ACM BVP Digital Library'\u001b[0m/\n","'Activity 2 edit.docx'\n"," anuj.docx\n"," anuj.gdoc\n"," arduino_final.apk\n"," \u001b[01;34mBooks\u001b[0m/\n"," \u001b[01;34mCats\u001b[0m/\n"," centurion.jpg\n","'Certificates Evotech 3.0.gsheet'\n"," \u001b[01;34mCG\u001b[0m/\n","'Characterization and Identification of Emotions v2.gdoc'\n"," CHECK.TXT\n"," \u001b[01;34mClassroom\u001b[0m/\n","\u001b[01;34m'Colab Notebooks'\u001b[0m/\n","'Copy of <Template App Name Privacy Policy>.gdoc'\n","'Copy of Trivia Game Multiple Choice (1).gsheet'\n","'Copy of Trivia Game Multiple Choice.gsheet'\n","'Copy of WhatsApp Image 2018-07-18 at 8.58.43 PM.jpeg'\n"," Courerass.rtf.gdoc\n","\u001b[01;34m'CSP Final'\u001b[0m/\n","\u001b[01;34m'CSP Play'\u001b[0m/\n","'CURRICULUM VITAE.pdf'\n"," data_lm.pkl\n"," DECODER.gdoc\n"," deepak.jpg\n","\"Deepak's Resume.pdf\"\n"," DEMO-VIDE0.mp4\n","\u001b[01;34m'DevFest 19 '\u001b[0m/\n"," difference-equations-3.pdf\n"," \u001b[01;34mdocuments\u001b[0m/\n"," dp.jpg\n","'ECE-1 STLD lab xD.gdoc'\n","'efficient parking solution.pdf'\n","'email format (1).gsheet'\n","'email format (2).gsheet'\n","'email format (3).gsheet'\n","'email format.gsheet'\n","'Employee Absenteeism (1).pptx'\n","'Employee Absenteeism.pptx'\n"," eon.rar\n"," \u001b[01;34mevotech\u001b[0m/\n"," \u001b[01;34mEvotech_final\u001b[0m/\n"," \u001b[01;34meYantra\u001b[0m/\n"," eyantra1.mp4\n","'Github Campus Expert write-up.docx'\n","'Github Campus Expert write-up.gdoc'\n"," GoogleNews-vectors-negative300.bin\n"," HAPPY.CPP\n"," hermite.pdf\n"," hi_3500.ods\n","'hiruzen will of fire (hi no ishi).png'\n","'Identification and Characterization of Emotions.gslides'\n","'Identification and Characterization of Emotions v2 Mini.gslides'\n"," IMG_20170428_145338003.jpg\n"," IMG_20171211_190845854.jpg\n"," IMG_20200527_095129.jpg\n","'I Want to Eat Your Pancreas.pdf'\n"," k49-test-labels.npz\n"," k49-train-labels.npz\n"," khushi_collage.jpg\n","'Khusi SAMUN.gsheet'\n"," L-04_Hermite_Catmull_Rom.pdf\n"," magikarp.png\n","'main (1).py.gdoc'\n"," main.py.gdoc\n"," MangaMaster-Backup.json\n"," \u001b[01;34mmodels\u001b[0m/\n"," Naruto.jpg\n","\u001b[01;34m'Official Farwell'\u001b[0m/\n"," poster.psd\n"," Prob1.gdoc\n"," prob2.gdoc\n"," prob3.gdoc\n","'research papers.gsheet'\n"," \u001b[01;34mresume\u001b[0m/\n","'Resume (1).gdoc'\n"," Resume.gdoc\n","'Screenshot (38) (1).png'\n","'Screenshot (38).png'\n"," \u001b[01;34msent\u001b[0m/\n"," Sketch\n","'Sketch invite'\n"," speech_nisha.docx\n"," split_stones.cpp\n"," STLD_Assignment_3.pdf\n","'Sun Tzu (13-29-32).jpeg'\n","'<Template App Name Privacy Policy>.gdoc'\n","'time-table draft.docx'\n","'Traffic and accident management.rtf'\n","'Turbo C++ 4.0 Windows 7 Windows 8 64Bit Version (1).exe'\n","'Turbo C++ 4.0 Windows 7 Windows 8 64Bit Version.exe'\n"," \u001b[01;34munity\u001b[0m/\n","'Untitled document.gdoc'\n"," usa.zipx\n"," venusaur_final.apk\n","'WORLD RECORD AT IIT(HRITAM,APOORV,DEEPAK - XI A ).pptx'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1WDyOheZphOc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593947079563,"user_tz":-330,"elapsed":933,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"16506151-dda3-41c8-ac59-ccf66e7d174b"},"source":["cd sent"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/sent\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qn4VgJAwpj-N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593947086580,"user_tz":-330,"elapsed":3180,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"c0782db7-3c82-441f-fffa-ad3da27bc302"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mneg\u001b[0m/  \u001b[01;34mpos\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rGuyh7UWplI0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1593948360782,"user_tz":-330,"elapsed":372007,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"d6bd01d4-91d4-4942-a133-e7ac4d24f16e"},"source":["from string import punctuation\n","from os import listdir\n","from collections import Counter\n","from nltk.corpus import stopwords\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# remove remaining tokens that are not alphabetic\n","\ttokens = [word for word in tokens if word.isalpha()]\n","\t# filter out stop words\n","\tstop_words = set(stopwords.words('english'))\n","\ttokens = [w for w in tokens if not w in stop_words]\n","\t# filter out short tokens\n","\ttokens = [word for word in tokens if len(word) > 1]\n","\treturn tokens\n","\n","# load doc and add to vocab\n","def add_doc_to_vocab(filename, vocab):\n","\t# load doc\n","\tdoc = load_doc(filename)\n","\t# clean doc\n","\ttokens = clean_doc(doc)\n","\t# update counts\n","\tvocab.update(tokens)\n","\n","# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# add doc to vocab\n","\t\tadd_doc_to_vocab(path, vocab)\n","\n","# define vocab\n","vocab = Counter()\n","# add all docs to vocab\n","process_docs('neg', vocab, True)\n","process_docs('pos', vocab, True)\n","# print the size of the vocab\n","print(len(vocab))\n","# print the top words in the vocab\n","print(vocab.most_common(50))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["44276\n","[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('bad', 1248), ('could', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"772A8uKxqYnT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593948406126,"user_tz":-330,"elapsed":896,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"dfefc772-1982-4048-d440-4cfbe0522b2b"},"source":["# keep tokens with a min occurrence\n","min_occurane = 2\n","tokens = [k for k,c in vocab.items() if c >= min_occurane]\n","print(len(tokens))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["25767\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dAfQdGoOun19","colab_type":"code","colab":{}},"source":["# save list to file\n","def save_list(lines, filename):\n","\t# convert lines to a single blob of text\n","\tdata = '\\n'.join(lines)\n","\t# open file\n","\tfile = open(filename, 'w')\n","\t# write text\n","\tfile.write(data)\n","\t# close file\n","\tfile.close()\n","\n","# save tokens to a vocabulary file\n","save_list(tokens, 'vocab.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMjv2ORKvA-q","colab_type":"text"},"source":["Keras embeddings"]},{"cell_type":"code","metadata":{"id":"Q9Dff9vZvJBA","colab_type":"code","colab":{}},"source":["# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# load the vocabulary\n","vocab_filename = 'vocab.txt'\n","vocab = load_doc(vocab_filename)\n","vocab = vocab.split()\n","vocab = set(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ww0B6OxgvWm4","colab_type":"code","colab":{}},"source":["# turn a doc into clean tokens\n","def clean_doc(doc, vocab):\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# filter out tokens not in vocab\n","\ttokens = [w for w in tokens if w in vocab]\n","\ttokens = ' '.join(tokens)\n","\treturn tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMWRv9DLvemf","colab_type":"code","colab":{}},"source":["# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\tdocuments = list()\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# load the doc\n","\t\tdoc = load_doc(path)\n","\t\t# clean doc\n","\t\ttokens = clean_doc(doc, vocab)\n","\t\t# add to list\n","\t\tdocuments.append(tokens)\n","\treturn documents\n","\n","# load all training reviews\n","positive_docs = process_docs('pos', vocab, True)\n","negative_docs = process_docs('neg', vocab, True)\n","train_docs = negative_docs + positive_docs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FD9oNqWRvipU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":802},"executionInfo":{"status":"ok","timestamp":1593954811518,"user_tz":-330,"elapsed":198753,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"d4a8e770-237f-47df-b8b9-2e958dad0365"},"source":["from string import punctuation\n","from os import listdir\n","from numpy import array\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc, vocab):\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# filter out tokens not in vocab\n","\ttokens = [w for w in tokens if w in vocab]\n","\ttokens = ' '.join(tokens)\n","\treturn tokens\n","\n","# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\tdocuments = list()\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# load the doc\n","\t\tdoc = load_doc(path)\n","\t\t# clean doc\n","\t\ttokens = clean_doc(doc, vocab)\n","\t\t# add to list\n","\t\tdocuments.append(tokens)\n","\treturn documents\n","\n","# load the vocabulary\n","vocab_filename = 'vocab.txt'\n","vocab = load_doc(vocab_filename)\n","vocab = vocab.split()\n","vocab = set(vocab)\n","\n","# load all training reviews\n","positive_docs = process_docs('pos', vocab, True)\n","negative_docs = process_docs('neg', vocab, True)\n","train_docs = negative_docs + positive_docs\n","\n","# create the tokenizer\n","tokenizer = Tokenizer()\n","# fit the tokenizer on the documents\n","tokenizer.fit_on_texts(train_docs)\n","\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(train_docs)\n","# pad sequences\n","max_length = max([len(s.split()) for s in train_docs])\n","Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define training labels\n","ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n","\n","# load all test reviews\n","positive_docs = process_docs('pos', vocab, False)\n","negative_docs = process_docs('neg', vocab, False)\n","test_docs = negative_docs + positive_docs\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(test_docs)\n","# pad sequences\n","Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define test labels\n","ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n","\n","# define vocabulary size (largest integer value)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# define model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 100, input_length=max_length))\n","model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(10, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary())\n","# compile network\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n","# evaluate\n","loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 1317, 100)         2576800   \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 1310, 32)          25632     \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 655, 32)           0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 20960)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                209610    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 11        \n","=================================================================\n","Total params: 2,812,053\n","Trainable params: 2,812,053\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n"," - 15s - loss: 0.6907 - accuracy: 0.5361\n","Epoch 2/10\n"," - 15s - loss: 0.6647 - accuracy: 0.6122\n","Epoch 3/10\n"," - 15s - loss: 0.5210 - accuracy: 0.7794\n","Epoch 4/10\n"," - 15s - loss: 0.3623 - accuracy: 0.9539\n","Epoch 5/10\n"," - 15s - loss: 0.3112 - accuracy: 0.9839\n","Epoch 6/10\n"," - 15s - loss: 0.2854 - accuracy: 0.9978\n","Epoch 7/10\n"," - 15s - loss: 0.2696 - accuracy: 0.9972\n","Epoch 8/10\n"," - 15s - loss: 0.2554 - accuracy: 0.9994\n","Epoch 9/10\n"," - 15s - loss: 0.2435 - accuracy: 0.9994\n","Epoch 10/10\n"," - 15s - loss: 0.2325 - accuracy: 0.9994\n","Test Accuracy: 85.000002\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YeNdcInVH4a0","colab_type":"text"},"source":["Training our own embedding using word 2 vec"]},{"cell_type":"code","metadata":{"id":"LuLUUSc1H9Es","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1593955223965,"user_tz":-330,"elapsed":18265,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"42add7e6-c810-40e7-8266-57b834549714"},"source":["from string import punctuation\n","from os import listdir\n","from gensim.models import Word2Vec\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# turn a doc into clean tokens\n","def doc_to_clean_lines(doc, vocab):\n","\tclean_lines = list()\n","\tlines = doc.splitlines()\n","\tfor line in lines:\n","\t\t# split into tokens by white space\n","\t\ttokens = line.split()\n","\t\t# remove punctuation from each token\n","\t\ttable = str.maketrans('', '', punctuation)\n","\t\ttokens = [w.translate(table) for w in tokens]\n","\t\t# filter out tokens not in vocab\n","\t\ttokens = [w for w in tokens if w in vocab]\n","\t\tclean_lines.append(tokens)\n","\treturn clean_lines\n","\n","# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\tlines = list()\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# load and clean the doc\n","\t\tdoc = load_doc(path)\n","\t\tdoc_lines = doc_to_clean_lines(doc, vocab)\n","\t\t# add lines to list\n","\t\tlines += doc_lines\n","\treturn lines\n","\n","# load the vocabulary\n","vocab_filename = 'vocab.txt'\n","vocab = load_doc(vocab_filename)\n","vocab = vocab.split()\n","vocab = set(vocab)\n","\n","# load training data\n","positive_docs = process_docs('pos', vocab, True)\n","negative_docs = process_docs('neg', vocab, True)\n","sentences = negative_docs + positive_docs\n","print('Total training sentences: %d' % len(sentences))\n","\n","# train word2vec model\n","model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n","# summarize vocabulary size in model\n","words = list(model.wv.vocab)\n","print('Vocabulary size: %d' % len(words))\n","\n","# save model in ASCII (word2vec) format\n","filename = 'embedding_word2vec.txt'\n","model.wv.save_word2vec_format(filename, binary=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total training sentences: 58109\n","Vocabulary size: 25767\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"w6UdfK3gM0Cx","colab_type":"text"},"source":["using our own embedding and then using our own NN"]},{"cell_type":"code","metadata":{"id":"4cDwv3a3IkGv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":697},"executionInfo":{"status":"ok","timestamp":1593955678339,"user_tz":-330,"elapsed":159048,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"dd5912a9-029a-4047-8b14-d07338b0351c"},"source":["from string import punctuation\n","from os import listdir\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc, vocab):\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# filter out tokens not in vocab\n","\ttokens = [w for w in tokens if w in vocab]\n","\ttokens = ' '.join(tokens)\n","\treturn tokens\n","\n","# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\tdocuments = list()\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# load the doc\n","\t\tdoc = load_doc(path)\n","\t\t# clean doc\n","\t\ttokens = clean_doc(doc, vocab)\n","\t\t# add to list\n","\t\tdocuments.append(tokens)\n","\treturn documents\n","\n","# load embedding as a dict\n","def load_embedding(filename):\n","\t# load embedding into memory, skip first line\n","\tfile = open(filename,'r')\n","\tlines = file.readlines()[1:]\n","\tfile.close()\n","\t# create a map of words to vectors\n","\tembedding = dict()\n","\tfor line in lines:\n","\t\tparts = line.split()\n","\t\t# key is string word, value is numpy array for vector\n","\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n","\treturn embedding\n","\n","# create a weight matrix for the Embedding layer from a loaded embedding\n","def get_weight_matrix(embedding, vocab):\n","\t# total vocabulary size plus 0 for unknown words\n","\tvocab_size = len(vocab) + 1\n","\t# define weight matrix dimensions with all 0\n","\tweight_matrix = zeros((vocab_size, 100))\n","\t# step vocab, store vectors using the Tokenizer's integer mapping\n","\tfor word, i in vocab.items():\n","\t\tweight_matrix[i] = embedding.get(word)\n","\treturn weight_matrix\n","\n","# load the vocabulary\n","vocab_filename = 'vocab.txt'\n","vocab = load_doc(vocab_filename)\n","vocab = vocab.split()\n","vocab = set(vocab)\n","\n","# load all training reviews\n","positive_docs = process_docs('pos', vocab, True)\n","negative_docs = process_docs('neg', vocab, True)\n","train_docs = negative_docs + positive_docs\n","\n","# create the tokenizer\n","tokenizer = Tokenizer()\n","# fit the tokenizer on the documents\n","tokenizer.fit_on_texts(train_docs)\n","\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(train_docs)\n","# pad sequences\n","max_length = max([len(s.split()) for s in train_docs])\n","Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define training labels\n","ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n","\n","# load all test reviews\n","positive_docs = process_docs('pos', vocab, False)\n","negative_docs = process_docs('neg', vocab, False)\n","test_docs = negative_docs + positive_docs\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(test_docs)\n","# pad sequences\n","Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define test labels\n","ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n","\n","# define vocabulary size (largest integer value)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# load embedding from file\n","raw_embedding = load_embedding('embedding_word2vec.txt')\n","# get vectors in the right order\n","embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n","# create the embedding layer\n","embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n","\n","# define model\n","model = Sequential()\n","model.add(embedding_layer)\n","model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary())\n","# compile network\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n","# evaluate\n","loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 1317, 100)         2576800   \n","_________________________________________________________________\n","conv1d_2 (Conv1D)            (None, 1313, 128)         64128     \n","_________________________________________________________________\n","max_pooling1d_2 (MaxPooling1 (None, 656, 128)          0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 83968)             0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 83969     \n","=================================================================\n","Total params: 2,724,897\n","Trainable params: 148,097\n","Non-trainable params: 2,576,800\n","_________________________________________________________________\n","None\n","Epoch 1/10\n"," - 15s - loss: 0.7000 - accuracy: 0.5361\n","Epoch 2/10\n"," - 15s - loss: 0.6317 - accuracy: 0.6350\n","Epoch 3/10\n"," - 15s - loss: 0.5459 - accuracy: 0.7178\n","Epoch 4/10\n"," - 15s - loss: 0.4340 - accuracy: 0.8000\n","Epoch 5/10\n"," - 15s - loss: 0.3202 - accuracy: 0.8861\n","Epoch 6/10\n"," - 15s - loss: 0.2298 - accuracy: 0.9294\n","Epoch 7/10\n"," - 15s - loss: 0.1568 - accuracy: 0.9644\n","Epoch 8/10\n"," - 15s - loss: 0.1007 - accuracy: 0.9856\n","Epoch 9/10\n"," - 15s - loss: 0.0691 - accuracy: 0.9950\n","Epoch 10/10\n"," - 15s - loss: 0.0399 - accuracy: 1.0000\n","Test Accuracy: 56.000000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4izKH4OfNC-2","colab_type":"text"},"source":["using GloVe embedding and our own NN"]},{"cell_type":"code","metadata":{"id":"u07RIz16JYKA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":697},"executionInfo":{"status":"ok","timestamp":1593959178182,"user_tz":-330,"elapsed":171419,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"d16b400b-003d-4115-f05e-2ae9942e1fd0"},"source":["from string import punctuation\n","from os import listdir\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc, vocab):\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# filter out tokens not in vocab\n","\ttokens = [w for w in tokens if w in vocab]\n","\ttokens = ' '.join(tokens)\n","\treturn tokens\n","\n","# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\tdocuments = list()\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# load the doc\n","\t\tdoc = load_doc(path)\n","\t\t# clean doc\n","\t\ttokens = clean_doc(doc, vocab)\n","\t\t# add to list\n","\t\tdocuments.append(tokens)\n","\treturn documents\n","\n","# load embedding as a dict\n","def load_embedding(filename):\n","\t# load embedding into memory, skip first line\n","\tfile = open(filename,'r')\n","\tlines = file.readlines()\n","\tfile.close()\n","\t# create a map of words to vectors\n","\tembedding = dict()\n","\tfor line in lines:\n","\t\tparts = line.split()\n","\t\t# key is string word, value is numpy array for vector\n","\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n","\treturn embedding\n","\n","# create a weight matrix for the Embedding layer from a loaded embedding\n","def get_weight_matrix(embedding, vocab):\n","\t# total vocabulary size plus 0 for unknown words\n","\tvocab_size = len(vocab) + 1\n","\t# define weight matrix dimensions with all 0\n","\tweight_matrix = zeros((vocab_size, 100))\n","\t# step vocab, store vectors using the Tokenizer's integer mapping\n","\tfor word, i in vocab.items():\n","\t\tvector = embedding.get(word)\n","\t\tif vector is not None:\n","\t\t\tweight_matrix[i] = vector\n","\treturn weight_matrix\n","\n","# load the vocabulary\n","vocab_filename = 'vocab.txt'\n","vocab = load_doc(vocab_filename)\n","vocab = vocab.split()\n","vocab = set(vocab)\n","\n","# load all training reviews\n","positive_docs = process_docs('pos', vocab, True)\n","negative_docs = process_docs('neg', vocab, True)\n","train_docs = negative_docs + positive_docs\n","\n","# create the tokenizer\n","tokenizer = Tokenizer()\n","# fit the tokenizer on the documents\n","tokenizer.fit_on_texts(train_docs)\n","\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(train_docs)\n","# pad sequences\n","max_length = max([len(s.split()) for s in train_docs])\n","Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define training labels\n","ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n","\n","# load all test reviews\n","positive_docs = process_docs('pos', vocab, False)\n","negative_docs = process_docs('neg', vocab, False)\n","test_docs = negative_docs + positive_docs\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(test_docs)\n","# pad sequences\n","Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define test labels\n","ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n","\n","# define vocabulary size (largest integer value)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# load embedding from file\n","raw_embedding = load_embedding('glove.6B.100d.txt')\n","# get vectors in the right order\n","embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n","# create the embedding layer\n","embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n","\n","# define model\n","model = Sequential()\n","model.add(embedding_layer)\n","model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary())\n","# compile network\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n","# evaluate\n","loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 1317, 100)         2576800   \n","_________________________________________________________________\n","conv1d_4 (Conv1D)            (None, 1313, 128)         64128     \n","_________________________________________________________________\n","max_pooling1d_4 (MaxPooling1 (None, 656, 128)          0         \n","_________________________________________________________________\n","flatten_4 (Flatten)          (None, 83968)             0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 83969     \n","=================================================================\n","Total params: 2,724,897\n","Trainable params: 148,097\n","Non-trainable params: 2,576,800\n","_________________________________________________________________\n","None\n","Epoch 1/10\n"," - 15s - loss: 0.7417 - accuracy: 0.5472\n","Epoch 2/10\n"," - 15s - loss: 0.4825 - accuracy: 0.7861\n","Epoch 3/10\n"," - 15s - loss: 0.3026 - accuracy: 0.8922\n","Epoch 4/10\n"," - 15s - loss: 0.1392 - accuracy: 0.9911\n","Epoch 5/10\n"," - 16s - loss: 0.0707 - accuracy: 0.9983\n","Epoch 6/10\n"," - 15s - loss: 0.0325 - accuracy: 1.0000\n","Epoch 7/10\n"," - 15s - loss: 0.0177 - accuracy: 1.0000\n","Epoch 8/10\n"," - 15s - loss: 0.0114 - accuracy: 1.0000\n","Epoch 9/10\n"," - 15s - loss: 0.0082 - accuracy: 1.0000\n","Epoch 10/10\n"," - 15s - loss: 0.0063 - accuracy: 1.0000\n","Test Accuracy: 72.500002\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1ij4eydqQVi1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1593957764405,"user_tz":-330,"elapsed":78439,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"0fcd2014-06b0-4ced-ee55-bd3dfe377f41"},"source":["from gensim.models import KeyedVectors\n","model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"RJRzW-chRuwi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"error","timestamp":1593959301177,"user_tz":-330,"elapsed":5942,"user":{"displayName":"Deepak Dhingra","photoUrl":"","userId":"17237261125144258365"}},"outputId":"843ed5f4-01d6-4686-e5e9-448eee46fd11"},"source":["from string import punctuation\n","from os import listdir\n","from numpy import array\n","from numpy import asarray\n","from numpy import zeros\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import Embedding\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","\n","# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc, vocab):\n","\t# split into tokens by white space\n","\ttokens = doc.split()\n","\t# remove punctuation from each token\n","\ttable = str.maketrans('', '', punctuation)\n","\ttokens = [w.translate(table) for w in tokens]\n","\t# filter out tokens not in vocab\n","\ttokens = [w for w in tokens if w in vocab]\n","\ttokens = ' '.join(tokens)\n","\treturn tokens\n","\n","# load all docs in a directory\n","def process_docs(directory, vocab, is_trian):\n","\tdocuments = list()\n","\t# walk through all files in the folder\n","\tfor filename in listdir(directory):\n","\t\t# skip any reviews in the test set\n","\t\tif is_trian and filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\tif not is_trian and not filename.startswith('cv9'):\n","\t\t\tcontinue\n","\t\t# create the full path of the file to open\n","\t\tpath = directory + '/' + filename\n","\t\t# load the doc\n","\t\tdoc = load_doc(path)\n","\t\t# clean doc\n","\t\ttokens = clean_doc(doc, vocab)\n","\t\t# add to list\n","\t\tdocuments.append(tokens)\n","\treturn documents\n","\n","# load embedding as a dict\n","def load_embedding(filename):\n","\t# load embedding into memory, skip first line\n","\tfile = open(filename,'r')\n","\tlines = file.readlines()\n","\tfile.close()\n","\t# create a map of words to vectors\n","\tembedding = dict()\n","\tfor line in lines:\n","\t\tparts = line.split()\n","\t\t# key is string word, value is numpy array for vector\n","\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n","\treturn embedding\n","\n","# create a weight matrix for the Embedding layer from a loaded embedding\n","def get_weight_matrix(embedding, vocab):\n","\t# total vocabulary size plus 0 for unknown words\n","\tvocab_size = len(vocab) + 1\n","\t# define weight matrix dimensions with all 0\n","\tweight_matrix = zeros((vocab_size, 100))\n","\t# step vocab, store vectors using the Tokenizer's integer mapping\n","\tfor word, i in vocab.items():\n","\t\tvector = embedding.get(word)\n","\t\tif vector is not None:\n","\t\t\tweight_matrix[i] = vector\n","\treturn weight_matrix\n","\n","# load the vocabulary\n","vocab_filename = 'vocab.txt'\n","vocab = load_doc(vocab_filename)\n","vocab = vocab.split()\n","vocab = set(vocab)\n","\n","# load all training reviews\n","positive_docs = process_docs('pos', vocab, True)\n","negative_docs = process_docs('neg', vocab, True)\n","train_docs = negative_docs + positive_docs\n","\n","# create the tokenizer\n","tokenizer = Tokenizer()\n","# fit the tokenizer on the documents\n","tokenizer.fit_on_texts(train_docs)\n","\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(train_docs)\n","# pad sequences\n","max_length = max([len(s.split()) for s in train_docs])\n","Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define training labels\n","ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n","\n","# load all test reviews\n","positive_docs = process_docs('pos', vocab, False)\n","negative_docs = process_docs('neg', vocab, False)\n","test_docs = negative_docs + positive_docs\n","# sequence encode\n","encoded_docs = tokenizer.texts_to_sequences(test_docs)\n","# pad sequences\n","Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","# define test labels\n","ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n","\n","# define vocabulary size (largest integer value)\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# load embedding from file\n","raw_embedding = load_embedding('glove.6B.100d.txt')\n","# get vectors in the right order\n","embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n","# create the embedding layer\n","embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n","\n","# define model\n","model = Sequential()\n","model.add(embedding_layer)\n","model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","print(model.summary())\n","# compile network\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# fit network\n","model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n","# evaluate\n","loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n","print('Test Accuracy: %f' % (acc*100))"],"execution_count":30,"outputs":[{"output_type":"error","ename":"UnicodeDecodeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-970634e4f3ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# load embedding from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mraw_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;31m# get vectors in the right order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0membedding_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_weight_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-970634e4f3ad>\u001b[0m in \u001b[0;36mload_embedding\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# load embedding into memory, skip first line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# create a map of words to vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x94 in position 19: invalid start byte"]}]},{"cell_type":"code","metadata":{"id":"uHvMqskoYKhu","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}